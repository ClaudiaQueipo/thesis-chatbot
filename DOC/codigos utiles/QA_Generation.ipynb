{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcZN4IoJYr3r"
      },
      "outputs": [],
      "source": [
        "!pip install langchain huggingface_hub tiktoken Chromadb pypdf sentence-transformers torch accelerate docx2txt\n",
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtMtYJXFN4bd"
      },
      "outputs": [],
      "source": [
        "# !huggingface-cli download TheBloke/zephyr-7B-alpha-GGUF zephyr-7b-alpha.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
        "!huggingface-cli download TheBloke/Barcenas-Mistral-7B-GGUF barcenas-mistral-7b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
        "# !huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q5_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
        "# !huggingface-cli download TheBloke/Mistral-7B-OpenOrca-oasst_top1_2023-08-25-v1-GGUF mistral-7b-openorca-oasst_top1_2023-08-25-v1.Q5_K_M.gguf --local-dir . --local-dir-use-symlinks False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de la localización del modelo.\n",
        "MODEL_PATH = \"./barcenas-mistral-7b.Q4_K_M.gguf\""
      ],
      "metadata": {
        "id": "3RSyddWnTxg1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UsW0BQq2W6OF"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "prompt_template_questions = \"\"\"Eres un experto en crear preguntas de práctica basadas en material de dominio general. Tu objetivo es escribir preguntas y respuestas a partir de un contexto dado. Lo haces haciendo preguntas sobre el texto a continuación:\n",
        "\n",
        "{text}\n",
        "Crea preguntas que prepararán a una persona para responderlas. Asegúrate de no perder ninguna información importante.\n",
        "\n",
        "PREGUNTAS:\"\"\"\n",
        "\n",
        "refine_template_questions = \"\"\" Eres un experto en crear preguntas de práctica basadas en material de estudio. Tu objetivo es ayudar a una persona a prepararse para responder estas preguntas. Hemos recibido algunas preguntas de práctica hasta cierto punto: {existing_answer}. Tenemos la opción de refinar las preguntas existentes o agregar nuevas. (solo si es necesario) con algo más de contexto a continuación.\n",
        "{text}\n",
        "Dado el nuevo contexto, refina las preguntas originales en español. Si el contexto no es útil, por favor proporciona las preguntas originales.\n",
        "\n",
        "PREGUNTAS: \"\"\"\n",
        "\n",
        "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template_questions, input_variables=[\"text\"])\n",
        "\n",
        "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
        "    input_variables=[\"existing_answer\", \"text\"],\n",
        "    template=refine_template_questions,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2lokjtMPXKCV"
      },
      "outputs": [],
      "source": [
        "# Se crea el reader para cargar el archivo, en este caso un pdf que debe subir al colab\n",
        "loader = PyPDFLoader(\"./example.pdf\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "8M1gd91ml9Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mev-5DxXCw-"
      },
      "outputs": [],
      "source": [
        "# Combine text from Document into one string for question generation\n",
        "text_question_gen = ''\n",
        "for page in data:\n",
        "   text_question_gen += page.page_content\n",
        "\n",
        "# Initialize Text Splitter for question generation\n",
        "# text_splitter_question_gen = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10) # Reduce chunk size\n",
        "text_splitter_question_gen = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=50)\n",
        "\n",
        "# Split text into chunks for question generation\n",
        "text_chunks_question_gen = text_splitter_question_gen.split_text(text_question_gen)\n",
        "\n",
        "# Convert chunks into Documents for question generation\n",
        "docs_question_gen = [Document(page_content=t) for t in text_chunks_question_gen]\n",
        "\n",
        "llm_question_gen = LlamaCpp(model_path=MODEL_PATH,temperature=0.75,top_p=1,verbose=True,n_ctx=4096)\n",
        "# llm_question_gen = LlamaCpp(model_path=\"./barcenas-mistral-7b.Q4_K_M.gguf\",temperature=0.1,top_p=1,verbose=True,n_ctx=2048, streaming = True)\n",
        "\n",
        "# Initialize question generation chain\n",
        "question_gen_chain = load_summarize_chain(llm=llm_question_gen, chain_type=\"refine\", verbose=True,\n",
        "                                question_prompt=PROMPT_QUESTIONS, refine_prompt=REFINE_PROMPT_QUESTIONS)\n",
        "\n",
        "# Run question generation chain\n",
        "questions = question_gen_chain.run(docs_question_gen)\n",
        "\n",
        "\n",
        "# Create vector database for answer generation\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\"})\n",
        "\n",
        "# Initialize vector store for answer generation\n",
        "vector_store = Chroma.from_documents(docs_question_gen, embeddings)\n",
        "\n",
        "\n",
        "llm_answer_gen = LlamaCpp(model_path=MODEL_PATH,temperature=0.75,top_p=1,verbose=True,n_ctx=4096)\n",
        "\n",
        "# Initialize retrieval chain for answer generation\n",
        "answer_gen_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, chain_type=\"stuff\",\n",
        "                                                retriever=vector_store.as_retriever(k=2))\n",
        "\n",
        "  # Split generated questions into a list of questions\n",
        "question_list = questions.split(\"\\n\")\n",
        "\n",
        "# Create a directory for storing answers\n",
        "answers_dir = os.path.join(tempfile.gettempdir(), \"answers\")\n",
        "os.makedirs(answers_dir, exist_ok=True)\n",
        "\n",
        "# Create a single file to save questions and answers\n",
        "qa_file_path = os.path.join(answers_dir, \"questions_and_answers.txt\")\n",
        "\n",
        "with open(qa_file_path, \"w\") as qa_file:\n",
        "        # Answer each question and save to the file\n",
        "        for idx, question in enumerate(question_list):\n",
        "            answer = answer_gen_chain.run(question)\n",
        "            qa_file.write(f\"Question {idx + 1}: {question}\\n\")\n",
        "            qa_file.write(f\"Answer {idx + 1}: {answer}\\n\")\n",
        "            qa_file.write(\"--------------------------------------------------\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nota Importante\n",
        "Debe subir un archivo pdf a colab y pasarselo a este codigo\n",
        "```python\n",
        "loader = PyPDFLoader(\"./example.pdf\")\n",
        "```\n",
        "para que le funcione correctamente.\n",
        "\n",
        "# Referencias\n",
        "[Repositorio de referencia](https://github.com/InsightEdge01/SmartExamApp/tree/master)\n"
      ],
      "metadata": {
        "id": "UBPjOl7oFo2u"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}